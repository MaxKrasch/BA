\chapter{Background}

For a better understanding of the project, this section shows where reinforcement learning is to be classified in machine learning and how reinforcement learning works. In addition the related work this thesis is based on will be discussed.

\section{Taxonomy of Machine Learning}

\begin{figure} 
	\centering
	\includegraphics[scale=0.2]{img/Logo/classification.png}
	\caption{Classification of machine learning}
	\label{fig:classifi}
\end{figure}

In machine learning a algorithm gets created that is able to learn from data you provide for the algorithm.
In 1997, Tom Mitchell gave the following popular definition for machine learning "A Computer program is said to learn from an experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E."\cite{shanthamallu2017brief}. This means a machine learning algorithm is able to learn by data you provide for the algorithm. The learning process itself contains two main steps: the first step is training a learning model by giving it training samples as input data and the second one is to test the learning model \cite{nasteski2017overview}. The validation of the training differs by the type of machine learning. There are basically three different types of machine learning, as shown in figure \ref{fig:classifi}:

Supervised Learning: In supervised learning the training data consists "true" labels for the input samples.  The algorithm is trained by  making predictions on the input data and improves its estimate using the true labels. The algorithm's estimate gets improved by optimizing a cost function, that is typically measured by the error between the algorithm estimates and the true labels.  The goal is to minimize the cost function in order to get as close as possible to the estimate being equal to the true labels. \cite{shanthamallu2017brief}

Unsupervised Learning: The samples in the training data set don't have corresponding labels. The goal is to draw inferences from the input data and build a model that describes the hidden structure and distribution of the input data, in order to learn something about the data.\cite{shanthamallu2017brief}

Reinforcement learning is the type of machine learning that is used in this thesis and is illustrated in detail in the next section.

\section{Reinforcement Learning}

As shown in figure ... reinforcement learning contains an agent, which is connected to its environment. The agent can observe the environment and take actions in it. On each step of interaction, the agent receives some indication of the current state of the environment as an input. Then the agent chooses and takes an action, what changes the state of the environment, what in return gives the agent a scalar reward signal. The goal of the agent is to learn the optimal policy to choose the best action in every state in order to maximize the long time cumulative reward. There are numerous different algorithms that can train the agent. Depending on the task, the observation state and the action state the optimal algorithm to take differs.  \cite{kaelbling1996reinforcement} Figure ... shows a list of the most popular algorithms and their required observation and action space. In the next chapter an algorithm based on twin delayed policy optimization will implemented.

\begin{equation}
	Q(s=2)
	\label{formel63}
\end{equation}
In the equation \ref{formel63}, Q is shown.

\section{Related Work and Target Definition}

The idea of letting an reinforcement learning agent solve tasks in real world environments or games is not a new one. Richard Bellman's paper about dynamic programming \cite{bellman1957dynamic} laid the foundation of reinforcement learning and Richard S. Sutton has essentially advanced the research with his works about temporal difference learning \cite{sutton1988learning} and policy gradient methods with function approximation \cite{sutton1999policy}. Deep Mind's paper about training a agent by deep reinforcement learning based on a variant of Q-learning to play Atari 2600 games and beating the score of expert players in three different games \cite{mnih2013playing} proved that reinforcement learning can achieve great results by getting high-dimensional sensory input. After this breakthrough a lot of research was done to build different reinforcement learning algorithms to successfully solve tasks in real world simulation environments and robotics, like twin delayed deep deterministic policy gradient by Fujimoto et al \cite{fujimoto2018}, which is an improvement of the deep deterministic policy gradient by Lillicrap et al. \cite{lillicrap2015continuous}, which itself is based on the deterministic policy gradient algorithm by Silva et al \cite{silver2014deterministic} combined with deep learning. Haarnoja et al used maximum entropy reinforcement learning \cite{haarnoja2018soft} and also gained great results.
Today there are a lot of algorithms that perform well in certain environments, where TD3 by Fujimoto et al and SAC by Haarnoja showed the best performance in most continuous control problems. 

In this thesis a algorithm based on TD3 will be implemented to make a two dimensional robot walk and jump in a continuous action environment and furthermore an analysis will be done on how choosing different reward functions changes the performance of the agent's learning speed and the robots walking and jumping techniques.

